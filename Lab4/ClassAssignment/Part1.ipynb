{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Part-of-speech tagging\n",
    "\n",
    "In this lab, we will start with part-of-speech (POS) tagging and chunking, which can be called _shallow parsing_ techniques. You will see some ways of analyzing the contents of corpora using these methods.\n",
    "\n",
    "First of all, you need to import the NLTK library and some of its resources. Run the code by pressing Ctrl-Enter inside the cell, as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nltk\n",
    "import nltk\n",
    "nltk.download(['punkt', 'averaged_perceptron_tagger', 'tagsets', 'gutenberg', 'brown'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Tokenization\n",
    "\n",
    "In order to use a POS tagger, you need to work with tokenized text. That is, the input to the POS tagger must be a list of strings, where every string is one, separate word token. NLTK has a built-in word tokenizer, which can be used as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"In order to use a POS tagger, you need to work with tokenized text.\"\n",
    "tokenized = nltk.word_tokenize(sentence)\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see when you run the code above, punctuation is separated out as their own tokens. Now it is your turn to **modify the sentence** and test the tokenization on some more challenging data:\n",
    "* What happens to apostrophes and hyphens inside words, in words such as *part-of-speech* or *don't*?\n",
    "* What happens to other types of punctuation, such as dot-dot-dot or double quotes etc?\n",
    "* What happens to abbreviations and titles in front of names, such as *Mr.* or *Prof.*, that end in a dot?\n",
    "* Can you find examples, in which the tokenizer fails to tokenize correctly?\n",
    "\n",
    "### 1.2 POS tagging\n",
    "\n",
    "Next, apply the POS tagger that is available in NLTK on your tokenized text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.pos_tag(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a list of tuples, that is, pairs of words and their predicted POS tag. Did the tagger tag your sentence correctly?\n",
    "\n",
    "If you prefer to see the tagging in a more readable form, you can use the following command: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(nltk.tuple2str(tup) for tup in nltk.pos_tag(tokenized)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can make it even cleaner, if you produce one single string as output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" \".join(nltk.tuple2str(tup) for tup in nltk.pos_tag(tokenized)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 What do the POS tags mean?\n",
    "\n",
    "In order to know whether the POS tagging is correct, you need to understand what the tags mean in the first place. You can ask NLTK for help on the meaning of specific tags, which is convenient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset(\"JJ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use regular expressions to find information about more tags at the same time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset(\"NN.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you want information on all possible POS tags, find out what regular expression you should use.\n",
    "\n",
    "### 1.4 Disambiguation of POS tags in context\n",
    "\n",
    "Next, take a moment to study how well the NLTK POS tagger manages to tag words with ambiguous parts of speech, in sentences, such as _\"They **refuse** to **permit** us to obtain the **refuse permit**.\"_ or _\"**Can** I have a **can** of milk, please?\"_\n",
    "\n",
    "All the necessary code is collected in the cell below, so that you don't have to click through many cells each time you test a new sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence = \"Enter your sentence here!\"\n",
    "tokenized = nltk.word_tokenize(sentence)\n",
    "pos_tagged = nltk.pos_tag(tokenized)\n",
    "print(\" \".join(nltk.tuple2str(tup) for tup in pos_tagged))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Tokenized and tagged corpora in NLTK\n",
    "\n",
    "NLTK contains some corpora that are already pre-tokenized, such as the _Gutenberg corpus_. NLTK also contains some corpora that are both pre-tokenized and pre-tagged, such as the _Brown corpus_. When the POS tags are already available, it means that (in principle) they are verified by linguists to be correct.\n",
    "\n",
    "Let us first look at the Gutenberg corpus. It contains the following texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(nltk.corpus.gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain some specific tokenized text from the Gutenberg corpus, we do like this. Here below, we only print the 99 first words of the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "print(tokenized[0:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you produce POS tags for the 99 first words of Jane Austen's Emma?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagged = # what goes here?\n",
    "\n",
    "print(\" \".join(nltk.tuple2str(tup) for tup in pos_tagged))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think about the quality of the POS tagging?\n",
    "\n",
    "In contrast to the Gutenberg corpus, the Brown corpus comes as both tokenized and tagged. The code snippet below demonstrates how to obtain word tokens as well as correctly POS tagged word tokens for the 100 first words of the Brown corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_100 = nltk.corpus.brown.words()[0:100]\n",
    "print(\"TOKENIZED:\", list(tokenized_100))\n",
    "print()\n",
    "\n",
    "pos_tagged_100 = nltk.corpus.brown.tagged_words()[0:100]\n",
    "print(\"POS TAGGED:\", \" \".join(nltk.tuple2str(tup) for tup in pos_tagged_100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the tags in the Brown corpus sometimes end in `-TL`, which indicates that the word occurs in a title. There are other suffixes as well: `-HL` means headline and `-NC` means citation. Also in some other respects the tag set differs from the one used by the NLTK POS tagger. An explanation of the POS tags in the pre-tagged Brown corpus can be found in [Wikipedia](https://en.wikipedia.org/wiki/Brown_Corpus).\n",
    "\n",
    "If you now run the NLTK POS tagger on the 100 first words of the tokenized Brown corpus, how does that tagging compare to the correct one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tagged_100_v2 = # Add your command here\n",
    "print(\"POS TAGGED 2:\", \" \".join(nltk.tuple2str(tup) for tup in pos_tagged_100_v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are done with Part 1 and can continue to Part 2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
