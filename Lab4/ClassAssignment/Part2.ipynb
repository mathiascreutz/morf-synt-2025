{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Chunking\n",
    "\n",
    "Now that you know how to tokenize and POS tag a text, you will learn how to build a _chunker_ that utilizes the POS tagging. The chunker is a useful tool for data mining and information extraction. For instance, you can search for specific, linguistically interesting patterns in a corpus. First, import NLTK and the necessary resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nltk\n",
    "import nltk\n",
    "nltk.download(['punkt', 'averaged_perceptron_tagger', 'brown'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 How to write a chunker\n",
    "\n",
    "Below, you can see how a sentence is first tokenized and POS tagged, and then we run a very simple chunker that finds specific types of noun phrases (NPs) in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The little yellow dog barked at the cat.\"\n",
    "tokenized = nltk.word_tokenize(sentence)\n",
    "pos_tagged = nltk.pos_tag(tokenized)\n",
    "\n",
    "chunk_grammar = \"NP: { <DT>? <JJ>* <NN> }\"\n",
    "chunk_parser = nltk.RegexpParser(chunk_grammar)\n",
    "result = chunk_parser.parse(pos_tagged)\n",
    "print(result.pformat(parens='[]'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can copy-paste the bracketed structure into the syntax tree generator, if you want to visualize it: http://mshang.ca/syntree/\n",
    "\n",
    "You can see from the tree why this is called _shallow_ parsing.\n",
    "\n",
    "The chunk grammar is written using **regular expressions**. First you name the chunk (e.g., `NP`). Then inside curly brackets `{ ... }` you write a sequence of POS tags. Each POS tag must be placed inside angle brackets `<...>`. You can use regular expression syntax both inside the angle brackets (if you want to match many different types of parts-of-speech) and outside the angle brackets (if you want to make some of the tags optional, for instance).\n",
    "\n",
    "You can have multiple conditions for the same chunk and you can identify many different types of chunks in the same grammar. The example below will clarify some further how to write chunk grammars in NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Rapunzel let down her long golden hair.\"\n",
    "tokenized = nltk.word_tokenize(sentence)\n",
    "pos_tagged = nltk.pos_tag(tokenized)\n",
    "\n",
    "# This chunk grammar is written on multiple lines, with comments for each rule\n",
    "chunk_grammar = r\"\"\"\n",
    "  NP: { <DT|PRP\\$>? <JJ>* <NN> }   # NP chunk: determiner/possessive, adjectives and noun\n",
    "  NP: { <NNP>+ }                   # NP chunk: sequences of proper nouns\n",
    "  VB: { <VB.*> <RP>? }             # VB chunk: some verb form optionally followed by a particle\n",
    "\"\"\"\n",
    "chunk_parser = nltk.RegexpParser(chunk_grammar)\n",
    "print(chunk_parser.parse(pos_tagged).pformat(parens='[]'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Using a chunker for information extraction\n",
    "\n",
    "If you use a larger data set, you can use your chunker as a data mining tool. Let us look for a particular type of expressions in the Brown corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the POS tagged Brown corpus\n",
    "pos_tagged = nltk.corpus.brown.tagged_words()\n",
    "\n",
    "# Define chunker and parse data with it\n",
    "chunk_grammar = r\"\"\"\n",
    "  CHUNK: { <VBN> <TO> <VB.*> <RP>? }   # Chunk: past participle verb + \"to\" + other verb + optional particle\n",
    "\"\"\"\n",
    "chunk_parser = nltk.RegexpParser(chunk_grammar)\n",
    "tree = chunk_parser.parse(pos_tagged)\n",
    "\n",
    "# Print all the matches in the data\n",
    "for subtree in tree.subtrees():\n",
    "    if subtree.label() == \"CHUNK\":\n",
    "        print(subtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is now to modify the chunk grammar in order to extract other types of chunks from the Brown corpus, for instance:\n",
    "* Extract more complicated noun phrases than in the toy examples above.\n",
    "* Extract chunks of some types of named entities (NER = named entity extraction).\n",
    "* Explore some types of verb - argument structures.\n",
    "\n",
    "When you are done here, continue to Part 3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
