{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. N-gram language models\n",
    "\n",
    "In Part 3 you computed frequencies of n-grams in a corpus. You also computed conditional frequencies for bigrams, that is, the number of times a specific word follows some other word.\n",
    "\n",
    "We now move on to statistical n-gram language models. In a statistical language model, plain frequencies are converted into probabilities. Since the implementation of such language models is a bit complicated, most of the program code that you need has been prepared for you in a module called `ngrams`. To begin with, you need to import both `nltk` and `ngrams`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nltk\n",
    "import nltk\n",
    "nltk.download(['punkt', 'gutenberg'])\n",
    "\n",
    "sys.path.append(\"../../../morf-synt-2025/src\")\n",
    "import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Plotting conditional n-gram probabilities\n",
    "\n",
    "Earlier on this course we have been talking about *smoothing*, a technique for transferring some probability mass from seen n-grams to unseen n-grams. This makes it possible to estimate a probability for new word sequences that do not occur in the training corpus but that might still occur in the language.\n",
    "\n",
    "In this lab, we will produce a smoothed model by mixing (interpolating) zero-, uni-, bi-, tri-, and fourgram probabilities. We have not mentioned **zerograms** before: a zerogram assigns the same (flat) probability to all words regardless of how frequent they are in the language; we also reserve some probability mass for unseen, so-called out-of-vocabulary (OOV), words.\n",
    "\n",
    "To make this a bit more concrete, let us estimate a language model (LM) from Jane Austen's Emma and plot the probabilities of the next word in a given word sequence. First we initialize the language model on text that has been converted to lower case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [w.lower() for w in nltk.corpus.gutenberg.words('austen-emma.txt')]\n",
    "lm = ngrams.Ngrams(tokenized)\n",
    "lm.set_weights(0.2, 0.2, 0.2, 0.2, 0.2) # more about weights further below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we ask the model for the probabilities of all possible words that could continue a given start of a sentence: _\"Emma forgot what to ...\"_. (Note that we lower-case this start of a sentence, too, so it will actually be _\"emma forgot what to\"_.) We then print the 10 most probable words and their probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = \"Emma forgot what to\"\n",
    "pdist = lm.sorted_pdist_ngram(nltk.word_tokenize(start.lower()))\n",
    "print(pdist[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot this probability distribution as a bar chart. Look at the plot. Do the probabilities make sense? What seems to be happening?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "n = 40  # number of most probable words to plot\n",
    "\n",
    "words = [w for w, _ in pdist[0:n]]\n",
    "freqs = [p for _, p in pdist[0:n]]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(range(n), freqs)\n",
    "plt.xticks(range(n), words, rotation=90)\n",
    "plt.title(start + \"...\")\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"Prob\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Adjusting the weights\n",
    "\n",
    "The language model that we are studying is a _mixture_ of five separate models: a zerogram, unigram, bigram, trigram, and fourgram model. Each model contributes to the final result by the _weight_ that is assigned to it. In the example above we assigned the same weight (= 0.2) to every model. So, each of the five models contributed by 20% to the end result.\n",
    "\n",
    "Your next task is to modify the weights:\n",
    "* What happens if you put all weight (1.0) on the zero-, uni-, bi-, tri-, or fourgram? Why?\n",
    "* What happens if you put most weight on some of the models? Why?\n",
    "* Is there an optimal configuration for the weights?\n",
    "\n",
    "Below you can find code for repeating the experiment with your own weights. Note that the sum of the weights must always be 1.0 (= 100%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the weights; their sum must be 1.0\n",
    "zerogram_weight = 0.2\n",
    "unigram_weight = 0.2\n",
    "bigram_weight = 0.2\n",
    "trigram_weight = 0.2\n",
    "fourgram_weight = 0.2\n",
    "\n",
    "# You can change the start of sentence, too:\n",
    "start = \"Emma forgot what to\"\n",
    "\n",
    "# This part you don't need to change\n",
    "lm.set_weights(zerogram_weight, unigram_weight, bigram_weight, trigram_weight, fourgram_weight)\n",
    "pdist = lm.sorted_pdist_ngram(nltk.word_tokenize(start.lower()))\n",
    "n = 40  # number of most probable words to plot\n",
    "words = [w for w, _ in pdist[0:n]]\n",
    "freqs = [p for _, p in pdist[0:n]]\n",
    "plt.figure()\n",
    "plt.bar(range(n), freqs)\n",
    "plt.xticks(range(n), words, rotation=90)\n",
    "plt.title(start + \"...\")\n",
    "plt.xlabel(\"Word\")\n",
    "plt.ylabel(\"Prob\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Perplexity\n",
    "\n",
    "One way to evaluate a language model is to calculate the overall probability it assigns to some test corpus that the model has _not_ been trained on. If the model gives a high probability to the test data, then the model has been able to predict the test data rather well, which usually means it is a good model. If the model gives a low probability to the test data, then the model has not been able to predict the data too well, and the model is bad, at least for this type of text.\n",
    "\n",
    "Rather than using probability as a measure, **perplexity** is typically used. Perplexity can be derived from the probability, and a good thing about perplexity is that it is not dependent on the _length_ of the test corpus. (If we calculate the probability of the test corpus, the longer the corpus, the lower the probability, usually.)\n",
    "\n",
    "Perplexity measures how \"perplex\", or \"confused\", or \"surprised\", our language model is by the test corpus. Perplexity is a positive number, such as 120. This number means that on average, the model has to guess between 120 equally good possible continuations. We want this value to be low, because that means that the model is fairly confident about what the next word should be.\n",
    "\n",
    "Let us compute the perplexity of our Emma LM on some test sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.set_weights(0.2, 0.2, 0.2, 0.2, 0.2) # reset the weights\n",
    "\n",
    "sent1 = \"She was the youngest of the two daughters of a most affectionate, indulgent father; \" + \\\n",
    "        \"and had, in consequence of her sister's marriage, been mistress of his house from a \" + \\\n",
    "        \"very early period.\"\n",
    "\n",
    "sent2 = \"The jury further said in term-end presentments that the City Executive Committee, which had \" + \\\n",
    "        \"over-all charge of the election, ''deserves the praise and thanks of the City of Atlanta'' for \" + \\\n",
    "        \"the manner in which the election was conducted.\"\n",
    "\n",
    "sent3 = \"Emma could not admit to herself that all that she dreamed of was that one day a handsome, rich, \" + \\\n",
    "        \"and particularly intelligent nobleman would ask her father for her hand in marriage.\"        \n",
    "        \n",
    "print(\"Perplexity of sentence 1:\", lm.perplexity(nltk.word_tokenize(sent1.lower())))\n",
    "print(\"Perplexity of sentence 2:\", lm.perplexity(nltk.word_tokenize(sent2.lower())))\n",
    "print(\"Perplexity of sentence 3:\", lm.perplexity(nltk.word_tokenize(sent3.lower())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you say about the perplexities, given the following additional information?\n",
    "* Sentence 1 is is straight from the novel, so it is part of the training corpus.\n",
    "* Sentence 2 is from the Brown corpus.\n",
    "* Sentence 3 is an invented sentence, inspired by Jane Austen.\n",
    "\n",
    "### 4.4 Generating text using an n-gram model\n",
    "\n",
    "In this last section we will use statistical language models to generate random text. The text is only partly random in the sense that words will be selected according to their n-gram probabilities. The more specific the model is the more \"real\" the language will appear.\n",
    "\n",
    "To start with, let us refresh our memory on what texts are available in the Gutenberg corpus. You can pick some other text than Jane Austen's Emma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available in the Gutenberg corpus:\", nltk.corpus.gutenberg.fileids())\n",
    "\n",
    "text = 'austen-emma.txt'  # you can change this\n",
    "tokenized = [w.lower() for w in nltk.corpus.gutenberg.words(text)]\n",
    "lm = ngrams.Ngrams(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next set the weights (or you can come back to this later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.set_weights(0.01, 0.1, 0.2, 0.3, 0.39) # you can change this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then generate a sentence (or something more or less like a sentence) at random. Every time you rerun this command, the sentence will change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lm.generate_sentence()\n",
    "print(\" \".join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also write your own beginning of the sentence and let the language model continue from that..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = \"Oh, I wish I could\"\n",
    "words = lm.generate_sentence(start=nltk.word_tokenize(start.lower()))\n",
    "print(\" \".join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out different corpora to train your model on, and also modify the weights.\n",
    "\n",
    "After this, you can continue to the home assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
