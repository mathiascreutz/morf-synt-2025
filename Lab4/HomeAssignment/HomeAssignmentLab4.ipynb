{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home assignment for Lab4: Extracting information from and about corpora\n",
    "\n",
    "This home assignment consists of two tasks. In the first task you will use a chunk parser to extract specific types of expressions from a corpus. In the second task you will use statistical language models to analyze a text snippet and what corpus it might originate from. The tasks are worth 2 points each.\n",
    "\n",
    "Remember to import the necessary libraries before starting to work on the tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install nltk\n",
    "import nltk\n",
    "nltk.download(['punkt_tab', 'averaged_perceptron_tagger_eng', 'gutenberg'])\n",
    "\n",
    "sys.path.append(\"../../../morf-synt-2025/src\")\n",
    "import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Expressions containing named entities\n",
    "\n",
    "In this task, you need to apply what you have learned in Parts 1 and 2 of the class assignment. You should extract *expressions that contain two different named entitites* from a text in the Gutenberg corpus in NLTK. You can pick whichever text from the Gutenberg corpus you like. You need to write your own chunk grammar and use a parser to locate all the chunks that your grammar recognizes.\n",
    "\n",
    "You can decide what kind of chunks you are looking for as long as the chunks correspond to expressions involving two different named entities. The relation between the two named entities can be marked using a preposition, a conjunction, a verb, a combination of multiple features, or whatever you come up with. Add a comment to your code what kind of expressions you are looking for. Have your program print all the chunks it finds. (Don't expect the result to be perfect, since the POS tagger is not perfect.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokenized text from the Gutenberg corpus\n",
    "tokenized = nltk.corpus.gutenberg.words('some-gutenberg-text.txt')\n",
    "\n",
    "# POS tag the tokenized text\n",
    "pos_tagged = # something\n",
    "\n",
    "# Define your chunker\n",
    "chunk_grammar = r\"\"\"\n",
    "  YOUR CHUNK GRAMMAR GOES HERE!\n",
    "\"\"\"\n",
    "\n",
    "# Parse the text with your chunker\n",
    "chunk_parser = nltk.RegexpParser(chunk_grammar)\n",
    "tree = chunk_parser.parse(pos_tagged)\n",
    "\n",
    "# Print all the matches in the data\n",
    "for subtree in tree.subtrees():\n",
    "    if subtree.label() == \"CHUNK\": # Rename CHUNK to whatever your chunk is called\n",
    "        print(subtree)\n",
    "\n",
    "# Add a comment here on what expressions you are\n",
    "# trying to find and how well your chunker works.\n",
    "# Remember that your expressions need to contain\n",
    "# two different named entities:        \n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Identification of text source\n",
    "\n",
    "This task is based on what you have learned about statistical language models in Part 4. Below you can see a sentence. You need to write appropriate program code that helps you figure out from which of the texts in the Gutenberg corpus the sentence might originate. Also add a comment to your code, where you explain your approach and what your conclusions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"She heard the Englishman talking, so she sold all she had; \" + \\\n",
    "       \"but still she fancied that the lucky day would really come \" + \\\n",
    "       \"sooner or later.\"\n",
    "\n",
    "# This code can be useful in your solution\n",
    "tokenized = [w.lower() for w in nltk.corpus.gutenberg.words('some-gutenberg-text.txt')]\n",
    "lm = ngrams.Ngrams(tokenized)\n",
    "\n",
    "# Also remember to set the weights\n",
    "\n",
    "# And how do you evaluate how well the given sentence\n",
    "# matches some text from the Gutenberg corpus?\n",
    "\n",
    "# Write a comment here, where you explain your approach and\n",
    "# your conclusions:\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done, download this page as a Notebook file (.ipynb) and submit the file on Moodle.\n",
    "\n",
    "Good luck with the assignments!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
