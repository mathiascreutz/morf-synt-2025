{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Beyond Toy Grammars\n",
    "\n",
    "The grammars we have worked on so far are super tiny. To get a grasp of something more realistic, let us train a probabilistic context free grammar (PCFG) from part of the Penn Treebank corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the NLTK library with the Penn treebank data\n",
    "import sys\n",
    "!{sys.executable} -m pip install nltk\n",
    "import nltk\n",
    "nltk.download('treebank')\n",
    "\n",
    "# Train a PCFG on data from a real treebank (this may take a while to run)\n",
    "productions = []\n",
    "for tree in nltk.corpus.treebank.parsed_sents():\n",
    "    productions += tree.productions()\n",
    "S = nltk.grammar.Nonterminal('S')\n",
    "grammar = nltk.induce_pcfg(S, productions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might be curious to see what kind of sentences there are in the training data for our parser. Output some sentences from the treebank by changing the sentence ID `sent_id` in the cell below. Plot the trees by copy-pasting into the syntax tree generator at http://mshang.ca/syntree/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_id = 0 # change this to some other integer numbers\n",
    "\n",
    "print(nltk.corpus.treebank.parsed_sents()[sent_id].pformat(parens='[]'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can parse some sentences of your own choice using the parser that you have just trained. Note that you need to use words that are in the vocabulary of the parser. Otherwise an error message will be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'let us meet tomorrow after lunch .' # change this sentence to some other sentences\n",
    "\n",
    "# Parse the sentence (this might take a while)\n",
    "viterbi_parser = nltk.ViterbiParser(grammar)\n",
    "trees = viterbi_parser.parse(sent.split())\n",
    "for tree in trees:\n",
    "    print(tree.pformat(parens='[]'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy-paste some of the parses into the syntax tree generator and look at the pictures. How well do you think the parser works?\n",
    "\n",
    "Next investigate the contents of the grammar. Print all productions (rules) that have a specific non-terminal in the left-hand side of the rule. Change the value of the non-terminal to a few different values. (You can see what kind of terminals exist in the grammar by looking at the parse trees that you have just produced.)\n",
    "\n",
    "Note that each production has a probability, because this is a _probabilistic_ context free grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_terminal_symbol = \"PRP\" # change this value to some other grammatical tags\n",
    "\n",
    "prods = [p for p in grammar.productions() if str(p.lhs()) == non_terminal_symbol]\n",
    "for (i, p) in enumerate(prods):\n",
    "    print(\"#{:d}:\".format(i), p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done with this part you can continue to the home assignment.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
